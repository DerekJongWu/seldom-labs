{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7dad2-a22f-4934-b1bd-e66d6dc1020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Crew, Process, Task, LLM\n",
    "from crewai.project import CrewBase, agent, crew, task\n",
    "from crewai_tools import WebsiteSearchTool, ScrapeWebsiteTool, TXTSearchTool\n",
    "from crewai.tools import BaseTool\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from crewai.tools import tool\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28550d1-7a23-4383-8338-c00a94c0f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "os.environ['SERPER_API_KEY'] = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d09bb-03d7-4013-b356-6dcf73104250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "llm=LLM(model=\"ollama/llama3\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6201467-af15-4ef1-92c6-9d23acbaece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"BrowserTool\")\n",
    "def scrape_and_summarize_website(source_url: str) -> str:\n",
    "    \"\"\"Useful to scrape and summarize a website content with financial news, blog, etc.\"\"\"\n",
    "    response = requests.get(source_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        relevant_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'div', 'section', 'article'])    \n",
    "        content = \"\\n\\n\".join([str(el) for el in relevant_elements])\n",
    "        content_chunks = [content[i:i + 8000] for i in range(0, len(content), 8000)]\n",
    "        summaries = []\n",
    "        for chunk in content_chunks:\n",
    "            agent = Agent(\n",
    "                role='Principal Researcher',\n",
    "                goal=\n",
    "                'Do amazing research and summaries based on the content you are working with',\n",
    "                backstory=\n",
    "                \"You're a Principal Researcher at a big company and you need to do research about a given topic.\",\n",
    "                allow_delegation=False,\n",
    "                llm=llm)\n",
    "            task = Task(\n",
    "                agent=agent,\n",
    "                description=\n",
    "                f'Analyze and summarize the content below, make sure to include the most relevant information in the summary, return only the summary nothing else.\\n\\nCONTENT\\n----------\\n{chunk}',\n",
    "                expected_output='A paragraph of the summary of the content provided. Should be detailed.'\n",
    "            )\n",
    "            summary = task.execute()\n",
    "            summaries.append(summary)\n",
    "        return \"\\n\\n\".join(summaries)\n",
    "    else:\n",
    "       return f\"Failed to fetch content from {source_url}. Status code: {response.status_code}\"\n",
    "\n",
    "@tool(\"InternetSearch\")\n",
    "def search_internet(query: str) -> str:\n",
    "    \"\"\"Useful to search the internet \n",
    "    about a a given topic and return relevant results\"\"\"\n",
    "    top_result_to_return = 4\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    payload = json.dumps({\"q\": query})\n",
    "    headers = {\n",
    "        'X-API-KEY': os.environ['SERPER_API_KEY'],\n",
    "        'content-type': 'application/json'\n",
    "    }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    results = response.json()['organic']\n",
    "    string = []\n",
    "    for result in results[:top_result_to_return]:\n",
    "      try:\n",
    "        string.append('\\n'.join([\n",
    "            f\"Title: {result['title']}\", f\"Link: {result['link']}\",\n",
    "            f\"Snippet: {result['snippet']}\", \"\\n-----------------\"\n",
    "        ]))\n",
    "      except KeyError:\n",
    "        next\n",
    "\n",
    "    return '\\n'.join(string)\n",
    "\n",
    "@tool(\"NewsSearch\")\n",
    "def search_news(query: str) -> str:\n",
    "    \"\"\"Useful to search news about a company, stock or any other\n",
    "    topic and return relevant results\"\"\"\"\"\n",
    "    top_result_to_return = 4\n",
    "    url = \"https://google.serper.dev/news\"\n",
    "    payload = json.dumps({\"q\": query})\n",
    "    headers = {\n",
    "        'X-API-KEY': os.environ['SERPER_API_KEY'],\n",
    "        'content-type': 'application/json'\n",
    "    }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    results = response.json()['news']\n",
    "    string = []\n",
    "    for result in results[:top_result_to_return]:\n",
    "      try:\n",
    "        string.append('\\n'.join([\n",
    "            f\"Title: {result['title']}\", f\"Link: {result['link']}\",\n",
    "            f\"Snippet: {result['snippet']}\", \"\\n-----------------\"\n",
    "        ]))\n",
    "      except KeyError:\n",
    "        next\n",
    "\n",
    "    return '\\n'.join(string)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aeeb81-e871-4edd-aff5-4fecc3940b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgents(): \n",
    "    def research_analyst(self): \n",
    "        return Agent(\n",
    "            role = \"The Best Research Assistant\",\n",
    "            goal = 'Impress all customers with your ability to gather and summarize information',\n",
    "            backstory = \"You are the BEST research analyst that is thorough and has a lot of experience gathering information and summarizing the most important parts for any topic given\",\n",
    "            verbose = True, \n",
    "            tools = [BrowserTools.scrape_and_summarize_website, \n",
    "                     SearchTools.search_news, \n",
    "                     SearchTools.search_internet, \n",
    "                     ScrapeWebsiteTool(), \n",
    "                     WebsiteSearchTool()], \n",
    "            llm = llm, \n",
    "            memory = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428ba73-a836-4e72-9254-8a76f8f6acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_assistant = Agent(\n",
    "    role = 'Research Assistant', \n",
    "    goal = 'Gather all the information you can given a certain topic', \n",
    "    backstory = 'You are the BEST research assistant that is thorough and has a lot of experience gathering information regarding any topic that is given', \n",
    "    verbose = True, \n",
    "    tools = [scrape_and_summarize_website, \n",
    "             search_internet,\n",
    "             search_news,\n",
    "             ScrapeWebsiteTool(), \n",
    "             WebsiteSearchTool()], \n",
    "    llm = llm, \n",
    "    memory = True\n",
    ")\n",
    "\n",
    "research_task = Task(\n",
    "    description = \"Collect and summarize news articles that discuss the 2018-2022 US administration's stance on tariffs\",\n",
    "    expected_output = 'Return a chronological report that summarizes everything that the governement intends to do with regards to tariffs. Make sure you also highlight the parties that the tariffs will be levied against' ,\n",
    "    agent = research_assistant,\n",
    "    async_execution = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee9948-48bb-44c8-b241-ca3de95ba275",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_crew = Crew(\n",
    "    agents = [research_assistant],\n",
    "    tasks = [research_task],\n",
    "    process = Process.sequential,\n",
    "    verbose = True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9abd1-fec5-4028-9822-156af3041562",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_crew.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66553f00-f70f-4a2f-bfe4-97fa364ebf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
